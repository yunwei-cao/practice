{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "# import Python packages as needed\n",
    "# NOTE: you can choose to install/use external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: word association mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_filepath = './amazon_reviews.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of reviews: 30000\n",
      "total number of positive reviews: 15091\n",
      "total number of negative reviews: 14909\n"
     ]
    }
   ],
   "source": [
    "# calculate the number of reviews\n",
    "num_reviews = 0\n",
    "num_positive_reviews = 0\n",
    "num_negative_reviews = 0\n",
    "with open(review_filepath) as f:\n",
    "    f.readline() # skip header (the first line) \n",
    "    for line in f:\n",
    "        num_reviews += 1\n",
    "        if line.strip().split()[0] == '1':\n",
    "            num_positive_reviews += 1\n",
    "        else:\n",
    "            num_negative_reviews += 1\n",
    "print ('total number of reviews:', num_reviews)\n",
    "print ('total number of positive reviews:', num_positive_reviews)\n",
    "print ('total number of negative reviews:', num_negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count frequency of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # replace punctuations with blank spaces\n",
    "    for punctuations in [',', '.', '\"', '!', '?', ':', ';', '-', '(', ')', '[', ']']:\n",
    "        text = text.replace(punctuations, ' ')\n",
    "    # eliminate duplicated whitespaces using wildcards\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # convert to lowercases\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of single words (aka. unigrams) in the corpus\n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "# Return: \n",
    "#       a dictionary, key = word, value = word frequency\n",
    "\n",
    "def get_single_word_frequency(filepath):\n",
    "    word_freq = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            # split tabs(arrows) at the beginning and the following reviews\n",
    "            review_text = process_text(line.split('\\t')[1])\n",
    "            # split reviews into single words\n",
    "            for word in review_text.split():\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                else:\n",
    "                    word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 119835\n",
      "and 64619\n",
      "i 63045\n",
      "a 60750\n",
      "to 57968\n",
      "it 47813\n",
      "of 47382\n",
      "this 44363\n",
      "is 41185\n",
      "in 27962\n"
     ]
    }
   ],
   "source": [
    "# list every single word and its frequency\n",
    "word_freq = get_single_word_frequency(review_filepath)\n",
    "\n",
    "# By using a key for sorted, we can sort the values according to the result of applying the key function to each value. Since lambda creates a callable (specifically, a function), we can use one for the key. Here the result is sorted in terms of the frequency, which the index is 1.\n",
    "for word, freq in sorted(word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 69037\n",
      "total number of word occurrences: 2384094\n"
     ]
    }
   ],
   "source": [
    "total_num_words = sum(word_freq.values())\n",
    "print ('number of unique words:', len(word_freq))\n",
    "print ('total number of word occurrences:', total_num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count frequency of ordered pair of words in a text window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of text windows that contain an ordered pair of words\n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "#       window_size: the size of a text window (measured in number of words)\n",
    "# Return: \n",
    "#       a dictionary, key = ordered word pair (a tuple), \n",
    "#                     value = number of text windows containing this pair\n",
    "\n",
    "def get_ordered_word_pair_frequency(filepath, window_size):\n",
    "    pair_freq = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            review_text = process_text(line.split('\\t')[1])\n",
    "            word_list = review_text.split()\n",
    "            for i in range(len(word_list)):\n",
    "                for j in range(i + 1, len(word_list)):\n",
    "                    # only consider pairs of words no more than window_size apart  \n",
    "                    if j - i + 1 >= window_size:\n",
    "                        break\n",
    "                    # put this ordered word pair into a tuple\n",
    "                    order_word_pair = (word_list[i], word_list[j])\n",
    "                    # accumulate counts\n",
    "                    if order_word_pair not in pair_freq:\n",
    "                        pair_freq[order_word_pair] = 1\n",
    "                    else:\n",
    "                        pair_freq[order_word_pair] += 1\n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 'the') 15208\n",
      "('the', 'of') 12728\n",
      "('to', 'the') 11803\n",
      "('this', 'is') 10832\n",
      "('the', 'the') 10615\n",
      "('and', 'the') 10479\n",
      "('in', 'the') 9092\n",
      "('the', 'and') 8728\n",
      "('the', 'is') 8420\n",
      "('is', 'a') 8106\n"
     ]
    }
   ],
   "source": [
    "TEXT_WINDOW_SIZE = 5\n",
    "pair_freq = get_ordered_word_pair_frequency(review_filepath, TEXT_WINDOW_SIZE)\n",
    "for pair, freq in sorted(pair_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(pair, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate pointwise mutual information for each ordered pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pointwise mutual information for this pair of words \n",
    "# PMI(xi, yj) = log P(X = xi, Y = yj)/ P(X = xi)P(Y = yj)\n",
    "\n",
    "WORD_PAIR_FREQUENCY_THRESHOLD = 50\n",
    "pmi_per_pair = {}\n",
    "for pair, freq in pair_freq.items():\n",
    "    if freq < WORD_PAIR_FREQUENCY_THRESHOLD: \n",
    "    # filter out infrequent word pairs\n",
    "        continue\n",
    "        \n",
    "    if pair[0] in word_freq and pair[1] in word_freq:\n",
    "        # freq: frequency of this word pair\n",
    "        # word_freq[pair[0]]: frequency of the first word in the pair, notice it is not pair_freq\n",
    "        # word_freq[pair[1]]: frequency of the second word in the pair, notice it is not pair_freq\n",
    "        # total_num_words: total number of words in the corpus (i.e. corpus size), notice it is word occurrence\n",
    "        Pxy = freq/total_num_words\n",
    "        Px = word_freq[pair[0]]/total_num_words\n",
    "        Py = word_freq[pair[1]]/total_num_words\n",
    "        pmi_per_pair[pair] = log(Pxy/(Px*Py))\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Pointwise Mutual Information---\n",
      "('blah', 'blah') 9.949864089900075\n",
      "('sci', 'fi') 9.696379184529272\n",
      "('hip', 'hop') 9.670191811272797\n",
      "('harry', 'potter') 9.626615142058197\n",
      "('stainless', 'steel') 9.42888259103066\n",
      "('blu', 'ray') 8.925332853777995\n",
      "('buyer', 'beware') 8.688618897930821\n",
      "('windows', 'xp') 8.453848288341772\n",
      "('tech', 'support') 7.970746600541408\n",
      "('web', 'site') 7.969335218781741\n"
     ]
    }
   ],
   "source": [
    "# sort word pairs in pmi_per_pair by their PMI from highest to lowest. Show the top 10 pairs.\n",
    "print('---Pointwise Mutual Information---')\n",
    "for pair, pmi in sorted(pmi_per_pair.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(pair, pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: from the result above I would say most of them make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: feature selection using Chi-square statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each word, count how many positive (negative) documents it appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of documents that has a specified sentiment and contain a single word  \n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "#       label: string '0' (negative) or '1' (positive).   \n",
    "# Return: \n",
    "#       a dictionary, key = word, value = word frequency\n",
    "\n",
    "def get_single_word_doc_frequency_per_label(filepath, label):\n",
    "    word_freq_per_label = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            sentiment_label = line.split('\\t')[0].strip()\n",
    "            if sentiment_label == label:\n",
    "                review_text = process_text(line.split('\\t')[1])\n",
    "                for word in set(review_text.split()):\n",
    "                    if word not in word_freq_per_label:\n",
    "                        word_freq_per_label[word] = 1\n",
    "                    else:\n",
    "                        word_freq_per_label[word] += 1\n",
    "    return word_freq_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 13245\n",
      "and 12526\n",
      "a 11891\n",
      "to 11156\n",
      "this 11082\n",
      "i 10334\n",
      "is 10106\n",
      "of 9998\n",
      "it 9793\n",
      "in 8182\n"
     ]
    }
   ],
   "source": [
    "# number of positive documents that contain a word \n",
    "positive_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '1')\n",
    "for word, freq in sorted(positive_word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 13615\n",
      "and 11743\n",
      "a 11675\n",
      "to 11463\n",
      "i 11415\n",
      "this 11379\n",
      "it 10358\n",
      "of 10128\n",
      "is 9420\n",
      "not 8236\n"
     ]
    }
   ],
   "source": [
    "# number of negative documents that contain a word \n",
    "negative_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '0')\n",
    "for word, freq in sorted(negative_word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Chi-square statistic for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table per word:\n",
    "#                                             sentiment\n",
    "#                       positive                            negative\n",
    "#               ------------------------------------------------------------------------\n",
    "#       present | word present, positive sentiment | word present, negative sentiment |  \n",
    "# word          ------------------------------------------------------------------------\n",
    "#       absent  | word absent,  positive sentiment | word absent, negative sentiment  |  \n",
    "#               ------------------------------------------------------------------------\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisqr_per_word = {}\n",
    "for word, freq in word_freq.items():\n",
    "    # filter infrequent words\n",
    "    if freq < 10:\n",
    "        continue\n",
    "    if word in positive_word_freq and word in negative_word_freq:        \n",
    "        # calculate the Chi-square statistic for this word\n",
    "        # use the following variables\n",
    "        # positive_word_freq[word]: number of positive reviews where this word is present\n",
    "        # negative_word_freq[word]: number of negative reviews where this word is present \n",
    "        # num_positive_reviews: number of positive reviews in total \n",
    "        # num_negative_reviews: number of negative reviews in total\n",
    "        # num_reviews: total number of reviews in the corpus\n",
    "        \n",
    "        # Observed count of cells\n",
    "        Obs_1_1 = positive_word_freq[word]\n",
    "        Obs_1_0 = negative_word_freq[word]\n",
    "        Obs_0_1 = num_positive_reviews - Obs_1_1\n",
    "        Obs_0_0 = num_negative_reviews - Obs_1_0\n",
    "        \n",
    "        # Expected count of cells\n",
    "        exp_1_1 = num_positive_reviews * (Obs_1_1 + Obs_1_0)/num_reviews\n",
    "        exp_1_0 = num_negative_reviews * (Obs_1_1 + Obs_1_0)/num_reviews\n",
    "        exp_0_1 = num_positive_reviews * (Obs_0_1 + Obs_0_0)/num_reviews\n",
    "        exp_0_0 = num_negative_reviews * (Obs_0_1 + Obs_0_0)/num_reviews\n",
    "        \n",
    "        # Summation \n",
    "        chi_1_1 = (Obs_1_1 - exp_1_1)**2/exp_1_1\n",
    "        chi_1_0 = (Obs_1_0 - exp_1_0)**2/exp_1_0\n",
    "        chi_0_1 = (Obs_0_1 - exp_0_1)**2/exp_0_1\n",
    "        chi_0_0 = (Obs_0_0 - exp_0_0)**2/exp_0_0\n",
    "        \n",
    "        chisqr_per_word[word] = chi_1_1 + chi_1_0 + chi_0_1 + chi_0_0\n",
    "        \n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Chi-Square---\n",
      "great 2259.208635662306\n",
      "not 2016.4733034985088\n",
      "waste 1300.8223017245875\n",
      "money 1204.9485759018141\n",
      "love 769.4901479541076\n",
      "best 757.4894506044782\n",
      "poor 667.987842854487\n",
      "worst 663.642219565316\n",
      "excellent 646.8180379179539\n",
      "disappointed 636.8233634415676\n"
     ]
    }
   ],
   "source": [
    "# sort words in chi2_per_word by their Chi-square value from highest to lowest. Show the top 10 words.\n",
    "# words that clearly associate with positive or negative sentiment\n",
    "print('---Chi-Square---')\n",
    "for word, chisqr in sorted(chisqr_per_word.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, chisqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Chi-Square---\n",
      "all 2.6200474577313497\n",
      "seals 0.10348902656726948\n",
      "a 1.0553774185217923\n",
      "old 1.764604534350523\n",
      "one 2.185089539818895\n",
      "neighbors 1.3179408693942627\n",
      "neighbor 2.0476351063038174\n",
      "worth 0.7866421059068137\n",
      "chose 0.002385381442201088\n",
      "first 2.5235155922170582\n"
     ]
    }
   ],
   "source": [
    "# words that do not clearly associate with positive or negative sentiment\n",
    "print('---Chi-Square---')\n",
    "for word, chisqr in sorted(chisqr_per_word.items(), key = lambda x: x[1] < 3.841, reverse = True)[:10]:\n",
    "    print(word, chisqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null hypothesis：the certain word has no correlation with the sentiment.\n",
    "\n",
    "alternative hypothesis: the certain word has correlation with the sentiment.\n",
    "\n",
    "degree of freedom = 1 * 1 = 1, we set α = 0.05, the corresponding value of χ² = 3.841\n",
    "\n",
    "For words with χ² > 3.841, the null hypothesis is not true. we have 95% confidence to say they are correlated with the sentiment.\n",
    "\n",
    "For words with χ² < 3.841, we can not turn down the null hypothesis. we have 95% confidence to say they are not correlated with the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Chi-Square---\n",
      "great 2259.208635662306\n",
      "not 2016.4733034985088\n",
      "waste 1300.8223017245875\n",
      "money 1204.9485759018141\n",
      "love 769.4901479541076\n",
      "best 757.4894506044782\n",
      "poor 667.987842854487\n",
      "worst 663.642219565316\n",
      "excellent 646.8180379179539\n",
      "disappointed 636.8233634415676\n"
     ]
    }
   ],
   "source": [
    "# another way to do the chisqr test by using the stats package \n",
    "# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)\n",
    "from scipy import stats\n",
    "\n",
    "chisqr_per_word = {}\n",
    "for word, freq in word_freq.items():\n",
    "    # filter infrequent words\n",
    "    if freq < 10:\n",
    "        continue\n",
    "    if word in positive_word_freq and word in negative_word_freq:          \n",
    "        # Observed count of cells\n",
    "        Obs_1_1 = positive_word_freq[word]\n",
    "        Obs_1_0 = negative_word_freq[word]\n",
    "        Obs_0_1 = num_positive_reviews - Obs_1_1\n",
    "        Obs_0_0 = num_negative_reviews - Obs_1_0\n",
    "        \n",
    "        observed = [[Obs_1_1,Obs_1_0],[Obs_0_1,Obs_0_0]]\n",
    "        out = stats.chi2_contingency(observed=observed,correction=False)\n",
    "        chisqr_per_word[word] = out[0]\n",
    "        continue\n",
    "\n",
    "print('---Chi-Square---')\n",
    "for word, chisqr in sorted(chisqr_per_word.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, chisqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: spell correction using letter n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the file\n",
    "a_list_filepath = './enwiktionary.a.list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new object at the end of the list, remove line breaks from data \n",
    "a_list = []\n",
    "with open(a_list_filepath) as f:\n",
    "    for line in f:\n",
    "        a_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words/phrases in the list: 305868\n"
     ]
    }
   ],
   "source": [
    "print ('number of words/phrases in the list:', len(a_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent a string with a set of n-grams\n",
    "def chunk_word_into_letter_ngrams(word, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(word)-n+1):\n",
    "        ngrams.append( word[i : i+n] )\n",
    "    return set(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o w', 'wor', 'rld', 'lo ', 'llo', ' wo', 'hel', 'orl', 'ell'}\n"
     ]
    }
   ],
   "source": [
    "print (chunk_word_into_letter_ngrams('hello world', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need a function that calculates the edit distance for any pair of words\n",
    "# (You can use an external package to calculate edit distance, e.g. the \"editdistance\" package)\n",
    "import editdistance\n",
    "\n",
    "def editdistance_sort(string,wordlist):\n",
    "    sort = {}\n",
    "    for word in wordlist:\n",
    "        sort[word] = editdistance.eval(string,word)\n",
    "    return sorted(sort.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---abreviation(Edit_tri)---\n",
      "[('abbreviation', 1), ('abbreviatio', 2), ('abbreviations', 2), ('alleviation', 2), ('abbreviationi', 2), ('abbreviatione', 2), ('abreviaron', 2), ('adbreviatio', 2), ('adbreviationi', 2), ('adbreviatione', 2)]\n",
      "\n",
      "---abstrictiveness(Edit_tri)---\n",
      "[('abstractiveness', 1), ('absorptiveness', 3), ('attractiveness', 3), ('abortiveness', 4), ('abstersiveness', 4), ('abstractedness', 4), ('abstractness', 4), ('assertiveness', 4), ('abstrictions', 4), ('attributiveness', 4)]\n",
      "\n",
      "---accanthopterigious(Edit_tri)---\n",
      "[('acanthopterygious', 2), ('acanthopterous', 4), ('acanthopterygians', 4), ('acanthopterygian', 5), ('acanthopterygii', 5), ('acanthopodious', 6), ('acanthophorous', 6), ('acanthopteri', 6), ('acanthopterans', 6), ('acanthocarpous', 7)]\n",
      "\n",
      "---artifitial inteligwnse(Edit_tri)---\n",
      "[('artificial intelligence', 4), ('artificial intelligences', 5), ('artificial life', 9), ('artificial insemination', 9), ('artificialities', 9), ('artificial horizons', 9), ('artificial persons', 9), ('artificial abortions', 9), ('artificial language', 10), ('adaptive intelligence', 10)]\n",
      "\n",
      "---agglumetation(Edit_tri)---\n",
      "[('agglomeration', 2), ('agglutination', 3), ('agglomerations', 3), ('argumentation', 3), ('agglomeratioun', 3), ('acclimatation', 4), ('augmentation', 4), ('aggregation', 4), ('aggeneration', 4), ('agglomerative', 4)]\n"
     ]
    }
   ],
   "source": [
    "# For each given string, you need to find a list of 10 correctly-spelled words from enwiktionary.a.list \n",
    "# that have the _lowest_ edit distance to the given word\n",
    "\n",
    "print('---abreviation(Edit_tri)---')\n",
    "print(editdistance_sort('abreviation',a_list)[:10])\n",
    "print('\\n---abstrictiveness(Edit_tri)---')\n",
    "print(editdistance_sort('abstrictiveness',a_list)[:10]) \n",
    "print('\\n---accanthopterigious(Edit_tri)---')\n",
    "print(editdistance_sort('accanthopterigious',a_list)[:10]) \n",
    "print('\\n---artifitial inteligwnse(Edit_tri)---')\n",
    "print(editdistance_sort('artifitial inteligwnse',a_list)[:10])  \n",
    "print('\\n---agglumetation(Edit_tri)---')\n",
    "print(editdistance_sort('agglumetation',a_list)[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You also need a function that can calculate the Jaccard similarity for any pair of words\n",
    "def Jaccard_Similarity(A,B):\n",
    "    intersection = A & B\n",
    "    union = A | B\n",
    "    J = len(intersection)/len(union)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each given string, you need to find a list of 10 correctly-spelled words from enwiktionary.a.list\n",
    "# that have the _highest_ n-gram Jaccard similarity to the given word\n",
    "# Different lengths of the n-grams (i.e., different n) will likely produce a different list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: from the result below I would say the trigrams work best. (4-grams and 5-grams do not get the right result for the 5th word \"agglutination\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity-trigrams\n",
    "def Jaccard_tri(string, wordlist):\n",
    "    trigram_str = chunk_word_into_letter_ngrams(string,3)\n",
    "    sort = {}\n",
    "    for word in wordlist:\n",
    "        trigram_word = chunk_word_into_letter_ngrams(word, 3)\n",
    "        sort[word] = Jaccard_Similarity(trigram_str,trigram_word)\n",
    "    return sorted(sort.items(),key = lambda x: x[1],reverse = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---abreviation(Jacc_tri)---\n",
      "[('abbreviation', 0.7272727272727273), ('abbreviations', 0.6666666666666666), ('abbreviationi', 0.6666666666666666), ('abbreviatione', 0.6666666666666666), ('adbreviationi', 0.6666666666666666), ('adbreviatione', 0.6666666666666666), ('abbreviatio', 0.6363636363636364), ('adbreviatio', 0.6363636363636364), ('abreviativo', 0.6363636363636364), ('abreviativa', 0.6363636363636364)]\n",
      "\n",
      "---abstrictiveness(Jacc_tri)---\n",
      "[('abstractiveness', 0.625), ('activeness', 0.5), ('addictiveness', 0.5), ('astrictive', 0.5), ('abstriction', 0.4666666666666667), ('abstricting', 0.4666666666666667), ('astrictives', 0.4666666666666667), ('abstrict', 0.46153846153846156), ('abstrictions', 0.4375), ('activenesses', 0.4375)]\n",
      "\n",
      "---accanthopterigious(Jacc_tri)---\n",
      "[('acanthopterygious', 0.55), ('acanthopteri', 0.5294117647058824), ('acanthopterous', 0.47368421052631576), ('acanthopteran', 0.42105263157894735), ('acanthopterans', 0.4), ('acanthopterygii', 0.38095238095238093), ('acanthopterygian', 0.36363636363636365), ('acanthopt', 0.35294117647058826), ('acanthopterygians', 0.34782608695652173), ('acanthopodious', 0.3333333333333333)]\n",
      "\n",
      "---artifitial inteligwnse(Jacc_tri)---\n",
      "[('artificial intelligence', 0.41379310344827586), ('artificial intelligences', 0.4), ('artificial insemination', 0.28125), ('artificial art', 0.24), ('artificial anus', 0.2222222222222222), ('artificial life', 0.2222222222222222), ('artificialmente', 0.2222222222222222), ('artificialities', 0.2222222222222222), ('artificial turf', 0.2222222222222222), ('artificial', 0.21739130434782608)]\n",
      "\n",
      "---agglumetation(Jacc_tri)---\n",
      "[('agglutination', 0.375), ('arietation', 0.35714285714285715), ('agglutinations', 0.35294117647058826), ('arietationi', 0.3333333333333333), ('arietatione', 0.3333333333333333), ('arietations', 0.3333333333333333), ('arietationes', 0.3125), ('arietationis', 0.3125), ('arietationum', 0.3125), ('arietationem', 0.3125)]\n"
     ]
    }
   ],
   "source": [
    "print('---abreviation(Jacc_tri)---')\n",
    "print(Jaccard_tri('abreviation',a_list)[:10]) \n",
    "print('\\n---abstrictiveness(Jacc_tri)---')\n",
    "print(Jaccard_tri('abstrictiveness',a_list)[:10]) \n",
    "print('\\n---accanthopterigious(Jacc_tri)---')\n",
    "print(Jaccard_tri('accanthopterigious',a_list)[:10]) \n",
    "print('\\n---artifitial inteligwnse(Jacc_tri)---')\n",
    "print(Jaccard_tri('artifitial inteligwnse',a_list)[:10])  \n",
    "print('\\n---agglumetation(Jacc_tri)---')\n",
    "print(Jaccard_tri('agglumetation',a_list)[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity-bigrams\n",
    "def Jaccard_bi(string, wordlist):\n",
    "    bigram_str = chunk_word_into_letter_ngrams(string,2)\n",
    "    sort = {}\n",
    "    for word in wordlist:\n",
    "        bigram_word = chunk_word_into_letter_ngrams(word,2)\n",
    "        sort[word] = Jaccard_Similarity(bigram_str,bigram_word)\n",
    "    return sorted(sort.items(),key = lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---abreviation(Jacc_bi)---\n",
      "[('abbreviation', 0.9090909090909091), ('abbreviations', 0.8333333333333334), ('abbreviationi', 0.8333333333333334), ('abbreviatione', 0.8333333333333334), ('abbreviatio', 0.8181818181818182), ('abbreviationis', 0.7692307692307693), ('abbreviationem', 0.7692307692307693), ('abbreviationes', 0.7692307692307693), ('abbreviationum', 0.7692307692307693), ('abbreviati', 0.7272727272727273)]\n",
      "\n",
      "---abstrictiveness(Jacc_bi)---\n",
      "[('abstractiveness', 0.75), ('astrictives', 0.6), ('activeness', 0.5333333333333333), ('abstivesen', 0.5333333333333333), ('astrictive', 0.5333333333333333), ('abstivesse', 0.5333333333333333), ('abstivesses', 0.5333333333333333), ('addictiveness', 0.5294117647058824), ('absorptiveness', 0.5), ('abstersiveness', 0.5)]\n",
      "\n",
      "---accanthopterigious(Jacc_bi)---\n",
      "[('acanthopterygious', 0.7368421052631579), ('acanthopterous', 0.6666666666666666), ('acanthopteri', 0.6470588235294118), ('acanthopteran', 0.5555555555555556), ('acanthopterygian', 0.55), ('acanthopterygii', 0.55), ('acanthopterans', 0.5263157894736842), ('acanthopterygians', 0.5238095238095238), ('acanthopodious', 0.5), ('acanthopt', 0.47058823529411764)]\n",
      "\n",
      "---artifitial inteligwnse(Jacc_bi)---\n",
      "[('artificial intelligence', 0.5555555555555556), ('artificial intelligences', 0.5357142857142857), ('artificial insemination', 0.42857142857142855), ('artificiality', 0.391304347826087), ('artificial life', 0.375), ('artificialities', 0.375), ('artificiali', 0.36363636363636365), ('artigliante', 0.36363636363636365), ('artificialia', 0.36363636363636365), ('artificialmente', 0.36)]\n",
      "\n",
      "---agglumetation(Jacc_bi)---\n",
      "[('agglutination', 0.5333333333333333), ('agglomeration', 0.5), ('agglutinations', 0.5), ('agglomerations', 0.47058823529411764), ('agglutinatarum', 0.47058823529411764), ('agglutinata', 0.4666666666666667), ('agitationum', 0.4666666666666667), ('agglomerationen', 0.4444444444444444), ('autoagglutination', 0.4444444444444444), ('agglutinaatio', 0.4375)]\n"
     ]
    }
   ],
   "source": [
    "print('---abreviation(Jacc_bi)---')\n",
    "print(Jaccard_bi('abreviation',a_list)[:10]) \n",
    "print('\\n---abstrictiveness(Jacc_bi)---')\n",
    "print(Jaccard_bi('abstrictiveness',a_list)[:10]) \n",
    "print('\\n---accanthopterigious(Jacc_bi)---')\n",
    "print(Jaccard_bi('accanthopterigious',a_list)[:10]) \n",
    "print('\\n---artifitial inteligwnse(Jacc_bi)---')\n",
    "print(Jaccard_bi('artifitial inteligwnse',a_list)[:10])  \n",
    "print('\\n---agglumetation(Jacc_bi)---')\n",
    "print(Jaccard_bi('agglumetation',a_list)[:10])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity-4grams\n",
    "def Jaccard_four(string, wordlist):\n",
    "    fourgram_str = chunk_word_into_letter_ngrams(string,4)\n",
    "    sort = {}\n",
    "    for word in wordlist:\n",
    "        fourgram_word = chunk_word_into_letter_ngrams(word, 4)\n",
    "        sort[word] = Jaccard_Similarity(fourgram_str,fourgram_word)\n",
    "    return sorted(sort.items(),key = lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---abreviation(Jacc_four)---\n",
      "[('abbreviation', 0.7), ('abbreviations', 0.6363636363636364), ('abbreviationi', 0.6363636363636364), ('abbreviatione', 0.6363636363636364), ('adbreviationi', 0.6363636363636364), ('adbreviatione', 0.6363636363636364), ('abbreviatio', 0.6), ('adbreviatio', 0.6), ('abreviativo', 0.6), ('abreviativa', 0.6)]\n",
      "\n",
      "---abstrictiveness(Jacc_four)---\n",
      "[('abstractiveness', 0.5), ('addictiveness', 0.4666666666666667), ('activeness', 0.46153846153846156), ('astrictive', 0.46153846153846156), ('abstriction', 0.42857142857142855), ('abstricting', 0.42857142857142855), ('astrictives', 0.42857142857142855), ('abstrict', 0.4166666666666667), ('abstrictions', 0.4), ('activenesses', 0.4)]\n",
      "\n",
      "---accanthopterigious(Jacc_four)---\n",
      "[('acanthopteri', 0.5), ('acanthopterygious', 0.45), ('acanthopteran', 0.3888888888888889), ('acanthopterous', 0.3684210526315789), ('acanthopterans', 0.3684210526315789), ('acanthopterygii', 0.35), ('acanthopterygian', 0.3333333333333333), ('acanthopterygians', 0.3181818181818182), ('acanthopt', 0.3125), ('acanthopts', 0.29411764705882354)]\n",
      "\n",
      "---artifitial inteligwnse(Jacc_four)---\n",
      "[('artificial intelligence', 0.3), ('artificial intelligences', 0.2903225806451613), ('artificial insemination', 0.18181818181818182), ('artificial art', 0.15384615384615385), ('artificial anus', 0.14814814814814814), ('artificial life', 0.14814814814814814), ('artificial turf', 0.14814814814814814), ('artifiko', 0.14285714285714285), ('artifice', 0.14285714285714285), ('artifici', 0.14285714285714285)]\n",
      "\n",
      "---agglumetation(Jacc_four)---\n",
      "[('arietation', 0.3076923076923077), ('arietationi', 0.2857142857142857), ('arietatione', 0.2857142857142857), ('arietations', 0.2857142857142857), ('arietationes', 0.26666666666666666), ('arietationis', 0.26666666666666666), ('arietationum', 0.26666666666666666), ('arietationem', 0.26666666666666666), ('agglutination', 0.25), ('agglutinations', 0.23529411764705882)]\n"
     ]
    }
   ],
   "source": [
    "print('---abreviation(Jacc_four)---')\n",
    "print(Jaccard_four('abreviation',a_list)[:10]) \n",
    "print('\\n---abstrictiveness(Jacc_four)---')\n",
    "print(Jaccard_four('abstrictiveness',a_list)[:10]) \n",
    "print('\\n---accanthopterigious(Jacc_four)---')\n",
    "print(Jaccard_four('accanthopterigious',a_list)[:10]) \n",
    "print('\\n---artifitial inteligwnse(Jacc_four)---')\n",
    "print(Jaccard_four('artifitial inteligwnse',a_list)[:10]) \n",
    "print('\\n---agglumetation(Jacc_four)---')\n",
    "print(Jaccard_four('agglumetation',a_list)[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity-5grams\n",
    "def Jaccard_five(string, wordlist):\n",
    "    fivegram_str = chunk_word_into_letter_ngrams(string,5)\n",
    "    sort = {}\n",
    "    for word in wordlist:\n",
    "        fivegram_word = chunk_word_into_letter_ngrams(word,5)\n",
    "        sort[word] = Jaccard_Similarity(fivegram_str,fivegram_word)\n",
    "    return sorted(sort.items(),key = lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---abreviation(Jacc_five)---\n",
      "[('abbreviation', 0.6666666666666666), ('abbreviations', 0.6), ('abbreviationi', 0.6), ('abbreviatione', 0.6), ('adbreviationi', 0.6), ('adbreviatione', 0.6), ('abbreviatio', 0.5555555555555556), ('adbreviatio', 0.5555555555555556), ('abreviativo', 0.5555555555555556), ('abreviativa', 0.5555555555555556)]\n",
      "\n",
      "---abstrictiveness(Jacc_five)---\n",
      "[('addictiveness', 0.42857142857142855), ('activeness', 0.4166666666666667), ('astrictive', 0.4166666666666667), ('abstriction', 0.38461538461538464), ('abstricting', 0.38461538461538464), ('astrictives', 0.38461538461538464), ('abstractiveness', 0.375), ('abstrict', 0.36363636363636365), ('abstrictions', 0.35714285714285715), ('activenesses', 0.35714285714285715)]\n",
      "\n",
      "---accanthopterigious(Jacc_five)---\n",
      "[('acanthopteri', 0.4666666666666667), ('acanthopteran', 0.35294117647058826), ('acanthopterygious', 0.35), ('acanthopterous', 0.3333333333333333), ('acanthopterans', 0.3333333333333333), ('acanthopterygii', 0.3157894736842105), ('acanthopterygian', 0.3), ('acanthopterygians', 0.2857142857142857), ('acanthopt', 0.26666666666666666), ('acanthopts', 0.25)]\n",
      "\n",
      "---artifitial inteligwnse(Jacc_five)---\n",
      "[('artificial intelligence', 0.23333333333333334), ('artificial intelligences', 0.22580645161290322), ('artificial insemination', 0.12121212121212122), ('artifiko', 0.1), ('artifice', 0.1), ('artifici', 0.1), ('artifizi', 0.1), ('artificer', 0.09523809523809523), ('artifical', 0.09523809523809523), ('artifices', 0.09523809523809523)]\n",
      "\n",
      "---agglumetation(Jacc_five)---\n",
      "[('arietation', 0.25), ('arietationi', 0.23076923076923078), ('arietatione', 0.23076923076923078), ('arietations', 0.23076923076923078), ('arietationes', 0.21428571428571427), ('arietationis', 0.21428571428571427), ('arietationum', 0.21428571428571427), ('arietationem', 0.21428571428571427), ('arietationibus', 0.1875), ('agitation', 0.16666666666666666)]\n"
     ]
    }
   ],
   "source": [
    "print('---abreviation(Jacc_five)---')\n",
    "print(Jaccard_five('abreviation',a_list)[:10]) \n",
    "print('\\n---abstrictiveness(Jacc_five)---')\n",
    "print(Jaccard_five('abstrictiveness',a_list)[:10]) \n",
    "print('\\n---accanthopterigious(Jacc_five)---')\n",
    "print(Jaccard_five('accanthopterigious',a_list)[:10]) \n",
    "print('\\n---artifitial inteligwnse(Jacc_five)---')\n",
    "print(Jaccard_five('artifitial inteligwnse',a_list)[:10])  \n",
    "print('\\n---agglumetation(Jacc_five)---')\n",
    "print(Jaccard_five('agglumetation',a_list)[:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
