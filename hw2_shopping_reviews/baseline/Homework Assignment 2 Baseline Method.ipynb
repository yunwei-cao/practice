{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files, set up folder, put files into folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: ./train.tsv\n",
    "# test data:     ./test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                             review\n",
      "0          0  Leaks: Liss seems to be totally incompetent: m...\n",
      "1          1  Replacement Peeler: Loved my old one. Loaned i...\n",
      "2          0  Not what I was expecting: I chose to rate this...\n",
      "3          1  Watch face is hard to read: Although I don't o...\n",
      "4          0  Disappointing: I was eager to read this book s...\n",
      "...      ...                                                ...\n",
      "29991      1  Love EW: I must admit that I am a total TV afi...\n",
      "29992      1  Easy to follow and delicious recipes!: I compl...\n",
      "29993      1  The Beauty and Mystery of Veronique: Perhaps t...\n",
      "29994      1  I love it.: Brilliant, hilarious, quick and ea...\n",
      "29995      0  broken...: bad choice...2d film would not play...\n",
      "\n",
      "[29996 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('./train.tsv', sep = '\\t')\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 23997\n",
      "validation set size: 5999\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8 # 80% for training, 20% for validation\n",
    "random_seed = 100\n",
    "\n",
    "train_dataframe = dataframe.sample(frac=train_ratio, random_state=random_seed)\n",
    "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
    "print('training set size:', len(train_dataframe))\n",
    "print('validation set size:', len(valid_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                             review\n",
      "0        1  Human Hurricane!: Would you like to sleep in t...\n",
      "1        2  A Mom: I bought this with all kinds of expecta...\n",
      "2        3  Good Read: I judge all books that I read by a ...\n",
      "3        4  It's awesome: DVD set is exactly what you'd bu...\n",
      "4        5  Great Movie!!!: This definatly the best Godzil...\n",
      "...    ...                                                ...\n",
      "5995  5996  Beautiful and Spiritual: This is a very beauti...\n",
      "5996  5997  Another Cash In: This cd is pure dreck and it'...\n",
      "5997  5998  Concept drawings-very good: The concept drawin...\n",
      "5998  5999  I hear i all the time is awsome: this is great...\n",
      "5999  6000  Not so great Performance: This mouse is very s...\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.read_csv('./test.tsv', sep = '\\t')\n",
    "print (test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the trivial baseline: predict the majority label of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 11965, 1: 12032})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority guess accuracy: 0.5099183197199533\n"
     ]
    }
   ],
   "source": [
    "# Looks like label 1 has slightly more counts than label 0 in training data\n",
    "# So the 'majority guess' prediction is an array filled with 1s\n",
    "majority_guess_pred = [1 for i in range(len(valid_dataframe))]\n",
    "accuracy = accuracy_score(valid_dataframe['label'], majority_guess_pred)\n",
    "print ('Majority guess accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: write out prediction values into a csv format file\n",
    "# params:\n",
    "#     df: dataframe, where each row is a test example, with column 'id' as data id\n",
    "#     pred: a list or 1-d array of prediction values\n",
    "#     filepath: the output file path\n",
    "# return:\n",
    "#     None\n",
    "\n",
    "def write_test_prediction(df, pred, filepath):\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write('{},{}\\n'.format('id', 'label'))\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write('{},{}\\n'.format(row['id'], pred[index]))\n",
    "    print (len(df), 'predictions are written to', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 predictions are written to ./majority_guess.csv\n"
     ]
    }
   ],
   "source": [
    "majority_guess_pred_test = [1 for i in range(len(test_dataframe))]\n",
    "write_test_prediction(test_dataframe, majority_guess_pred_test, './majority_guess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use all unigrams from training data as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_dataframe['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or: reuse the chi-square feature selection method from HW1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(text):\n",
    "    for punctuations in [',', '.', '\"', '!', '?', ':', ';', '-', '(', ')', '[', ']']:\n",
    "        text = text.replace(punctuations, ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def get_single_word_frequency(filepath):\n",
    "    word_freq = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            review_text = process_text(line.split('\\t')[1])\n",
    "            for word in review_text.split():\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                else:\n",
    "                    word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "def get_single_word_doc_frequency_per_label(filepath, label):\n",
    "    word_freq_per_label = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            sentiment_label = line.split('\\t')[0].strip()\n",
    "            if sentiment_label == label:\n",
    "                review_text = process_text(line.split('\\t')[1])\n",
    "                for word in set(review_text.split()):\n",
    "                    if word not in word_freq_per_label:\n",
    "                        word_freq_per_label[word] = 1\n",
    "                    else:\n",
    "                        word_freq_per_label[word] += 1\n",
    "    return word_freq_per_label\n",
    "\n",
    "def feature_selection_chi2(review_filepath, num_features_to_select):\n",
    "    num_reviews = 0\n",
    "    num_positive_reviews = 0\n",
    "    num_negative_reviews = 0\n",
    "    with open(review_filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            num_reviews += 1\n",
    "            if line.strip().split()[0] == '1':\n",
    "                num_positive_reviews += 1\n",
    "            else:\n",
    "                num_negative_reviews += 1\n",
    "    word_freq = get_single_word_frequency(review_filepath)\n",
    "    positive_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '1')\n",
    "    negative_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '0')\n",
    "    \n",
    "    chi2_dict = {}\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in positive_word_freq and word in negative_word_freq:        \n",
    "            contingency_table = np.zeros((2,2))\n",
    "            contingency_table[0][0] = positive_word_freq[word]\n",
    "            contingency_table[0][1] = negative_word_freq[word]\n",
    "            contingency_table[1][0] = num_positive_reviews - positive_word_freq[word]\n",
    "            contingency_table[1][1] = num_negative_reviews - negative_word_freq[word]\n",
    "\n",
    "            chi2 = 0.0\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    expected_count = sum(contingency_table[i,:])*sum(contingency_table[:,j])/float(num_reviews)\n",
    "                    chi2 += (contingency_table[i][j] - expected_count)**2 / expected_count\n",
    "\n",
    "            chi2_dict[word] = chi2\n",
    "    feature_set = set([])\n",
    "    for word, chi2 in sorted(chi2_dict.items(), key = lambda x: x[1], reverse = True)[:num_features_to_select]:\n",
    "        feature_set.add(word)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(vocabulary={'#', '#1', '$', '$$$', '$1', '$10', '$13', '$14',\n",
       "                            '$15', '$18', '$19', '$2', '$20', '$200', '$25',\n",
       "                            '$250', '$29', '$3', '$30', '$300', '$39', '$4',\n",
       "                            '$5', '$50', '$6', '$60', '$7', '$75', '$8', '$80', ...})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 5000\n",
    "feature_set = feature_selection_chi2('./train.tsv', num_features)\n",
    "vectorizer = CountVectorizer(vocabulary = feature_set)\n",
    "vectorizer.fit(train_dataframe['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature vectors for training, validation, and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23997, 5000)\n",
      "(5999, 5000)\n",
      "(6000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_X = vectorizer.transform(train_dataframe['review'])\n",
    "valid_X = vectorizer.transform(valid_dataframe['review'])\n",
    "test_X = vectorizer.transform(test_dataframe['review'])\n",
    "print (train_X.shape)\n",
    "print (valid_X.shape)\n",
    "print (test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, solver='liblinear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = train_dataframe['label']\n",
    "model = LogisticRegression(C = 1, solver='liblinear')\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on training set: 0.9446597491353086\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat = model.predict(train_X)\n",
    "train_Y = train_dataframe['label'].to_numpy()\n",
    "accuracy = accuracy_score(train_Y, train_Y_hat)\n",
    "print ('Logistic regression, accuracy on training set:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on validation set: 0.8836472745457576\n"
     ]
    }
   ],
   "source": [
    "valid_Y_hat = model.predict(valid_X)\n",
    "valid_Y = valid_dataframe['label'].to_numpy()\n",
    "accuracy = accuracy_score(valid_Y, valid_Y_hat)\n",
    "print ('Logistic regression, accuracy on validation set:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After experimentation on the validation set: retrain the final model on all training data, and predict labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 predictions are written to ./logistic_regression.csv\n"
     ]
    }
   ],
   "source": [
    "all_train_Y = dataframe['label']\n",
    "all_train_X = vectorizer.transform(dataframe['review'])\n",
    "model.fit(all_train_X, all_train_Y)\n",
    "test_Y_hat = model.predict(test_X)\n",
    "write_test_prediction(test_dataframe, test_Y_hat, './logistic_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate what the model has learned and where it failed (A.K.A. error analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at learned parameters (for linear model: weight of each dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a mapping: word -> learned weight of this word\n",
    "feature_weight = {}\n",
    "for word, idx in vectorizer.vocabulary_.items():\n",
    "    feature_weight[word] = model.coef_[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pleasantly 2.279402940495796\n",
      "refreshing 2.205919739088226\n",
      "worried 1.95732094393739\n",
      "mars 1.9460025065988507\n",
      "finest 1.8565518345759777\n",
      "neat 1.7924083430328102\n",
      "ch 1.7359182932270139\n",
      "adorable 1.7190764107097123\n",
      "gem 1.6675652240766863\n",
      "pleased 1.6588436574745422\n"
     ]
    }
   ],
   "source": [
    "# words correlated with positive sentiment (top ones)\n",
    "for k, v in sorted(feature_weight.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "     print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disappointing -2.722746977631435\n",
      "worthless -2.675360261781088\n",
      "waste -2.541643888760075\n",
      "worst -2.4806099150298264\n",
      "overrated -2.40984309457248\n",
      "alas -2.3494191985941586\n",
      "eh -2.181271452099388\n",
      "hopes -2.1732474653070297\n",
      "disappointment -2.1527476608010305\n",
      "disappointed -2.0185290153854245\n"
     ]
    }
   ],
   "source": [
    "# words correlated with negative sentiments (top ones)\n",
    "for k, v in sorted(feature_weight.items(), key = lambda x: x[1], reverse = False)[:10]:\n",
    "     print (k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how the model makes predictions on individual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick a set of examples from the validation set (we predicted scores for those).\n",
    "# We usually we don't pick from training data (since the good performance may be unrealistic).\n",
    "# We cannot do error analysis on test data （because no true target value is provided）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_linear_prediction(df, model, idx2feature, X, Y, Y_hat, idx_list):\n",
    "    print('indices:', idx_list)\n",
    "    for idx in idx_list:\n",
    "        print ('==============', idx, '================')\n",
    "        print ('document:', df.iloc[idx]['review'])\n",
    "        print ('TRUE label:', df.iloc[idx]['label'])\n",
    "        print ('PRED label:', Y_hat[idx])\n",
    "        \n",
    "        print ('\\nPRED breakdown:')\n",
    "        print ('\\tINTERCEPT', model.intercept_)\n",
    "        if X[idx, :].nnz == 0:\n",
    "            print ('\\tFEATURE', '[EMPTY]')\n",
    "        else:\n",
    "            sp_row = X[idx, :]\n",
    "            for i in range(sp_row.getnnz()): # looping over a row in sparse matrix \n",
    "                feature_value = sp_row.data[i]\n",
    "                feature_dim = sp_row.indices[i]\n",
    "                print ('\\tFEATURE', idx2feature[feature_dim], ':', feature_value, '*', model.coef_[0][feature_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [2245]\n",
      "============== 2245 ================\n",
      "document: Was not working...: The product did recieve power, but after buying a XD card not an SD card, we found it did not work. If it had worked it would have been a perfect for our child who wanted his first camera.The person we purchased from was excellent though. Shipping was very fast, and he had no problems processing our return. He even gave us a goodwill credit for our troubles.\n",
      "TRUE label: 0\n",
      "PRED label: 1\n",
      "\n",
      "PRED breakdown:\n",
      "\tINTERCEPT [0.09476012]\n",
      "\tFEATURE after : 1 * -0.1540410232075748\n",
      "\tFEATURE an : 1 * 0.024432241729830828\n",
      "\tFEATURE and : 1 * 0.1287635087116256\n",
      "\tFEATURE been : 1 * 0.11693673654496411\n",
      "\tFEATURE but : 1 * -0.058176839160452465\n",
      "\tFEATURE buying : 1 * -0.3735138160237207\n",
      "\tFEATURE card : 2 * 0.27507295795772446\n",
      "\tFEATURE child : 1 * -0.25639585399833775\n",
      "\tFEATURE credit : 1 * 0.13721045885374877\n",
      "\tFEATURE did : 2 * -0.03096315402592158\n",
      "\tFEATURE even : 1 * -0.3052928491885844\n",
      "\tFEATURE excellent : 1 * 1.5769780275912673\n",
      "\tFEATURE fast : 1 * 0.47780149226774743\n",
      "\tFEATURE first : 1 * 0.022030843121174704\n",
      "\tFEATURE for : 2 * -0.005778538875641538\n",
      "\tFEATURE from : 1 * 0.02937575246156344\n",
      "\tFEATURE gave : 1 * -0.2275777065627122\n",
      "\tFEATURE goodwill : 1 * -0.3959537256672201\n",
      "\tFEATURE had : 2 * 0.013949152860086654\n",
      "\tFEATURE have : 1 * 0.018231633767051596\n",
      "\tFEATURE he : 2 * -0.0828833453932448\n",
      "\tFEATURE his : 1 * -0.08780184905134207\n",
      "\tFEATURE if : 1 * -0.2943569670672771\n",
      "\tFEATURE it : 3 * 0.018813781793179385\n",
      "\tFEATURE no : 1 * -0.3619629629253528\n",
      "\tFEATURE not : 3 * -0.7984846081964957\n",
      "\tFEATURE our : 3 * 0.2571948543051187\n",
      "\tFEATURE perfect : 1 * 0.8933372924586668\n",
      "\tFEATURE problems : 1 * 0.3570055102459087\n",
      "\tFEATURE product : 1 * -0.021639098965364027\n",
      "\tFEATURE purchased : 1 * -0.09135464854885769\n",
      "\tFEATURE return : 1 * -1.266374401445916\n",
      "\tFEATURE shipping : 1 * 0.3543997969644024\n",
      "\tFEATURE the : 2 * -0.010568235191609368\n",
      "\tFEATURE troubles : 1 * 0.5053971300297121\n",
      "\tFEATURE us : 1 * 0.21547651366895434\n",
      "\tFEATURE very : 1 * 0.06605683068015845\n",
      "\tFEATURE wanted : 1 * 0.09839387609472701\n",
      "\tFEATURE was : 3 * -0.06506098922615919\n",
      "\tFEATURE we : 2 * 0.07468112546746104\n",
      "\tFEATURE who : 1 * 0.09069478374064281\n",
      "\tFEATURE work : 1 * -0.37188202893125466\n",
      "\tFEATURE worked : 1 * -0.38967769338538466\n",
      "\tFEATURE working : 1 * -0.0885637661793551\n",
      "\tFEATURE would : 1 * -0.18645809834081883\n"
     ]
    }
   ],
   "source": [
    "# construct a dictionary mapping: feature index -> word\n",
    "idx2feature = dict([(v,k) for k,v in vectorizer.vocabulary_.items()])\n",
    "\n",
    "# look at data with prediction error\n",
    "error_indices  = [i for i in range(len(valid_Y_hat)) if valid_Y_hat[i] != valid_Y[i]]\n",
    "explain_linear_prediction(valid_dataframe, model, idx2feature, valid_X, valid_Y, valid_Y_hat, np.random.choice(error_indices, size = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
